Channel Name: 2MinutesPy
Channel ID: UCbcD3tpv7kIQU2cp5F9KJNA

--- Combined Transcripts ---

--- Video 1 ---
Video ID: -TA7orMJuJ4
Video Title: Intuition and Math Behind KNN Classifier in ML Explained

Transcript:
K nearest neighbors one of the simplest algorithms used for classification and regression tasks it includes a whole bunch of classes but today we'll go with the K neighor classifier have you ever voted for someone that's pretty much how classification is done using the K neighor classifier by voting it decides a class for new data points based on the majority vote of its nearest neighbors and when we say nearest we're talking mathematically closest so how does this work in practice let's visualize it here's some data represented by red and blue dots suddenly a black dot appears it's a new unseen data point our task is to classify this point does it belong to the red class or the blue class the process starts by calculating the distance between the black dot and every point in our data set once we have all the distances we sort them in increasing order next we select the K closest points our nearest neighbors by default the value of K is five these five neighbors then vote to decide the class of the black dot the class with the majority votes wins in this case the majority of the neighbors are red so the black dot is classified as red simple right but how do we calculate those distances the K neighbor classifier uses a metric called the minkowski distance which is a flexible and generalized way to calculate distances the formula looks like this here the parameter P controls the type of distance let me show you what that means when p is set to one the manowski distance becomes the Manhattan distance it measures the distance along the grid lines when p is set to two the manowski distance becomes the ukian distance this is the straight line distance like using a ruler let's return to our data we can see that these points are placed on a two-dimensional plane with X and Y axis representing features so we marked their positions on the plane now we have some numbers on the screen we can now easily calculate the distance between the given point and every Point Let's calculate the distance using the Manhattan metric for this we set the hyperparameter P equal to 1 in this we calculate the sum of the absolute differences between the coordinates once we do this for all points we sort the distances in increasing order and pick the five closest neighbors the point is then classified on the majority vote next up the ukian metric this is the default behavior of the K neighbor classifier for this we Square the differences between the coordinates sum them up and take the square root once we do this for all points we sort the distances pick the closest Five and classify the point based on the majority vote in the case of larger P the manowski distance metric comes into play in this we calculate the absolute differences between the coordinates then we raise these differences to the power of P sum them up and then take the 1 over P root of the result after calculating distances for All Points we sort them take the vote from the K nearest neighbors and classify the point based on the majority that's it that was all about the working of the K neighbor classifier and the maths involved in it

