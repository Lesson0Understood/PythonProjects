Channel Name: Visually Explained
Channel ID: UCoTo2gtN527CXhe7jbP6hUg

--- Combined Transcripts ---

--- Video 1 ---
Video ID: qg4PchTECck
Video Title: Gradient Descent in 3 minutes

Transcript:
gradient descent is the process by which machines learn how to generate new faces play hide and seek and even beat the best humans at games like dota but what exactly is gradient descent to answer that question let's say you're trying to train your computer to listen to an audio file and recognize three spoken commands based on a labeled data set the first step is to formulate this machine learning task as a mathematical optimization problem for example you can work with a neural network whose weights are unknown variables whose input is an audio data and whose output is a vector of size 3 where each entry represents how much the neural network thinks the audio corresponds to each command then for each example in the data set you compare the output of the neural network to the ideal output take the difference the square and then the sum and you get the cost of a single training example by taking the sum over all training examples you get the overall cost function and the problem now becomes how do we find the right value of theta that makes this cost function as small as possible and this is where the second step or gradient descent comes in and you might ask why do we need to invent an algorithm for this at all can't we just plot the function and point to the minimizer well data usually has many more entries than just two and in that setting it's hard to see what the function looks like so what can we do the insight of gradient descent is that while we cannot get a look at the whole function all at once we can easily evaluate the cost function at an arbitrary point and with a process called back propagation we can also evaluate the negative gradient of this cos function and take a small step in that direction gradient descent continues in an iterative fashion it computes the negative gradient at the new point takes a step in that direction and so on and so forth the intuition behind every step is that the gradient of a function gives you the direction that increases that function the most so by taking a small step in the opposite direction you can hope to make the function decrease at each iteration gradient descent as i just presented it is known as vanilla gradient descent because several variants have been developed throughout the years to improve it in stochastic gradient descent instead of taking this sum over all training examples when taking the gradient of the cost function which can be expensive when the number of examples is big we can take a small random subset at each iteration adaptive gradient descent picks a different step size for each component of data and this can be extremely useful when data is parsed like text data and image data momentum gradient descent keeps track of previously computed gradients to build up momentum and accelerate conversions to the minimizer this was great in descent in under three minutes if you liked the video like and subscribe and see you next time [Music] you

