Channel Name: Active Learning
Channel ID: UCc8q7XD_EgmLtOYyfkYNa9w

--- Combined Transcripts ---

--- Video 1 ---
Video ID: XJEfX4fPrgY
Video Title: Linear Regression Math Intuition | Machine Learning 01

Transcript:
[Music] hey guys welcome back to another video i know i haven't uploaded in quite a while but we're back all right so in this video we're going to talk about linear regression which is probably one of the most basic machine learning algorithms that you can learn and while it might seem boring compared to some of the more modern models it's certainly quite important that you have a good grasp on and understanding on how linear regression works in order to understand the more complicated and difficult models all right so yeah also it's a good stepping stone into machine learning in general okay so let's get started uh on this on my screen i have a scatter plot of data points right and the data points show on the x-axis we have the house size and the y-axis we have the house price so what this means is that given a house size we can see how much it costs right for example this one uh i can see how much it cost right and then this green line fits all the data points in a linear fashion and we call this line the hypothesis function and this is basically linear regression right fitting a line to the data okay so what does this line actually tell us why do we need this line so for example if we know how much a house how big a house is right the house size we can simply do this for example you know how much how big it is we can draw up to here we can get a data point here and then since we know this what we know how much it costs right so this is the y value you know we have the x value of the line we can get the y value which is the house price so this is a good way to predict how much a household cost based on the house size right given a house size we can't predict how much it will cost right you know we can predict it for any house size right we can do it here you know we might have one here right it's super useful and it's not going to be a perfect estimation right it's still a good estimation for example this one it's not perfectly accurate but yeah it's useful now let's move on to the mathematics behind how this actually works all right so here's the general formula for linear regression and you can see here that is the hypothesis function right that we talked about earlier and you know we have theta and x so you might be wondering what is theta and what is x well x is what we call features in this case so what are features well features are fairly simple you know house size is a feature right the number of rooms in a house that's a feature number of fans number of swimming pools you know you get the point those are features right and so since wait let me write it out real quick these are features all right x and x values are features and all right we get that so what are theta uh what is theta then you know what are these uh well actually before we get to that um x0 is actually generally written at x0 is always one actually so in practice we don't actually have to write out x0 i'm just writing it to show you how how you can write it um to better understand right and so data is what we call feature weights right so what are feature ways well future ways are what machine learning is actually trying to pre uh trying to learn from right um the better the feature weights the more accurate the model will be right so that's where machine learning comes in and this term here this is called the bias term the let me write out bias turn and you can just disregard x0 since it's just one so it doesn't really matter right and these are the features x are features yeah okay so now we got that out of the way i i will talk about how you can optimize for data later we will use a technique called gradient descent which you know requires calculus so if you're not familiar with uh differentiation partial derivatives i highly suggest you to learn those first before you learn gradient descent right so all right let's move on um so here is the simple linear regression you might see this a lot from a lot of youtube videos um so what this is it's really simple it's if if i write it this way you might see it more clearly this is basically the same thing as y equal x plus b or c depending on where you're from um and so right it's just the equation for a line right you know theta one here is m x y here is x theta 0 is b or c depending on where you are right this is just the line the equation for the line and x 1 here um you know in our example it's you know our example back here it's actually the house size right how so it's something that we can use to estimate um how much the house will cost right cool all right um let's see yeah i think that's about it and we can also express this formula up here in vectorized form so what do i mean by that um here's what i mean by that so we can express theta as a vector containing values from x 0 all the way to x n and then x values all the way from x 0 to x n as well right x as vectors and then we can just do the dot product of theta and x um you know vectors and it's basically the same thing as theta 0 times x 0 theta 1 times x 1 theta 2 times x 2 and all the way up to theta n times x n so yeah that's pretty easy to understand right remember that this is called the hypothesis function all right cool and you know if you're wondering how can i implement this and python you can use the numpy library you can just do mp dot dot and data x right so it's fairly simple to implement if you're actually looking forward to implementing this all right cool now we got that done and now let's move on to how do we actually maximize uh how do we optimize our linear model right so you know we got the same data uh same scatter plot here except you see that i i added some white lines over here so what are those lines well those lines are called residuals let me write that down so what are residuals well residual is basically the difference between the predicted value and the actual value so in this case the predictive value is this point on the green line right that's what we predicted based on the house size for the house price right and then the actual value the actual house price and we subtract that from the predicted value and that's how we calculate the residuals right um but yeah we we use something called a cost function you or you can call it loss function doesn't really matter here and what cost functions are used for is evaluate model right value a model all right cool fantastic yeah you can also call a loss function it doesn't really matter cost loss function right so how do we do that well in this case we use a cos function called means uh actually let me do that in different colors like that's better we use a cost function called um mean squared error right and it's pretty self-explanatory is basically the mean of the sum of the error and the error in this case is uh you'll see i want to write down the formula so 1 over 2m times the sum and [Music] right so let me explain what this formula is so we're taking the sum of um y hat here it means the predicted value and you know we're taking the i predicted value of starting from 1 all the way to m m data points and then we're squaring that difference so why do we square that difference well number one it can get rid of negative signs right we don't want negative signs because it can't cancel out right between data points here so it's not good to have negative points and so you might ask why don't we just take the absolute value then well you'll see later on that we do this for a lot of different reasons one of them being that it's easier to well it's a lot easier to take the derivative of a function squared instead of absolute value right you know if we take the derivative of this we just bring two to the front and that's why we wrote a two here you know um we don't actually um include two typically well we do this so that it's easier to take the derivative of this function right it just cancels out but normally it's just um the mean of the sum of the squared errors right but we added two here so it's easier to cancel this out later on so yeah that's mean square there and let's see what else okay let's define this as j theta 0 theta 1. all right uh let's see well you can also actually write it this way technically you can't express white hat as h of the hypothetics function right and we're taking the i hypothesis function that's also a good way to write it right since we're plugging in x we get the y predicted value anyways so that's another way that you can express it okay all right now you might also see a [Music] cost function called root mean square root mean square error right so what's the difference between mean square and root mean square here well root mean square error is pretty self-explanatory it's actually just square root of mean squared there so why do we take the square root of that well if we look back at our scatter plot here and we use the mean square we can see that the unit we get at the end as house price squared because we square it here right and it wouldn't really make sense right you're it's a unit that doesn't really make sense house price squared right that's not a really good indicator so that's why we take the square root so now the unit is house price right that i feel like that's much more explainable right um but in practice we actually um don't need to use rm root mean square error when optimizing the machine learning model because taking the square root of this and using this is the same thing right we're trying to minimize this sorry i forgot to mention we're trying to minimize the cost function because you know we're trying to get um the least amount of error in terms of the difference between predictive values and actual values right so we're trying to minimize it and if we're trying to minimize this it's basically the same thing as minimizing this so it doesn't really matter in this case all right so now let's see how do we actually optimize um this line here how do we get the best data values so that our um function our um our cost functions are minimized right so that's called something uh well we're going to work with something called gradient descent right so let's see oops sorry my bad all right so we're optimizing seeing [Music] theta right and we're using gradient descent one sec [Music] [Music] so yeah we're using something called gradient this and if you took calculus you know that gradient is um similar to the slope right and descent means going downward and so it means with slope going downward and i'll explain later why this term makes a lot of sense right so what is the gradient descent used for so here i have uh theta zero if you don't remember what theta zero is let me go back up oh by the way i'm doing this for simple um simple linear regression this model here right so we only have theta zero and theta one right we know x one because you know we have house size right so we're good with that and we're trying to optimize data zero here right so you know how do we how do we get this curve right so um we got this curve by plugging in different values for theta zero right we might have a value here and then we get this value for mean square right so we by plugging different values for data zero we get different um mean square error uh or different cost function values right so and so what's the best data zero then you might ask well since we're trying to minimize uh mean square the best data value that theta zero value will be here right at the bottom of this curve right and that that will be the best state of zero the one that we're trying to get right since mean square is the lowest here so it'll make sense that we're trying to optimize for this all right so how do we actually do that well we use something called gradient descent so let's see how that works for example let let's start at x uh theta zero equals zero right so this point and then what we do is that we jump down here we gradually jump down here to this point and you know to this data zero value and then we do it again but this time we're taking smaller steps and gradient distance does this through mathematics right we'll see how this works and then take smaller steps smaller steps all the way until we reach this point right and then if it goes further it will just realize hey wait this point was lower and so it wouldn't make sense um yeah so that's what we do here and you know x0 is also the intercept so we're optimizing for the best intercept value here and how do we actually mathematically use gradient descent well let me show you here let's actually we can just move here yeah in fact that's better um we actually take the partial derivative of the function j right if you remember j uh theta zero theta one right j is the cost function here right remember what this is the cost function mean square and you know theta zero is the intercept for yeah the intercept value and then data one is the um feature weight right we're optimizing for these okay so then how do we opt so let's first optimize theta zeros that's what we're working with so what we do is we take the partial derivative j over partial [Music] theta zero and that's you know equal to partial derivative um let me write it out first it's easier this way and then our we write down our cost function 2m sum i equal 1 and theta 0 plus theta 1 x 1 ah x minus y i square right but yeah this is basically um what i'm using x i here because we're in plug in the x value well the i x value right it and then the i y value so we take the partial derivative of this and then what we get is we move two to the front right so two cancel out and then it's just divided by m okay um so i believe that would be one over m times i equal one and then since uh theta zero is actually not multiplied by anything we can just um actually wait hold on let me think for a second it's either this or yeah maybe we don't write that one over him yeah i'm not really sure for this but um maybe we don't write this um if you you have taken partial derivatives you can correct me here um but i don't think we take it [Music] i don't think we should include maybe we should i'm not sure but yeah and then theta 0 plus theta 1 x i minus y i and you know since the two cancel out we don't have to write that and we multiply by one which is the same as not multiplying by anything so why do we multiply by one well since one here is multiplied by theta zero so it's not shown and then that's why we multiply by one here right then we do something called step size uh yeah so you know so how do we get a better theta zero value well we take the data zero value and then we use gradient descent right um we multiplies by something called learning rate we multiply something called learning rate by uh what we just wrote you know the partial derivative right and for if you're wondering what learning rate is um it's basically a factor that determines how big of a step that you should take we typically start pretty big like 0.1 0.001 depending on the it really depends but yeah we in practice we start big and then um you know gradient descent will optimize it and you know oh sorry i didn't mean to cross that so by doing this we go down further and further right all right cool and then um the learning gravy gets smaller as slope gets less deep right and actually this whole thing is called the step size all right cool and [Music] now we actually do the same thing for data one as well um doing all these steps and we we do it all the way until we reach the best value where mean square is minimized right now what are some problems that we can run into well let me show you here let me start a new graph well we can't run into problems where it's called that's something called a local minima right so you know our graph doesn't have to necessarily look like this right it it almost never looks exactly like that and we might have something like this right so then we have two minimums right this is called the local minima this is called the global minima which is the lowest point but this is the local lowest point you can think of right and then you know with mse on the side you know data zero or theta one doesn't really matter here um yeah it so for a linear regression you actually optimize for all the theta using gradient descent right um sorry for the partial derivative um you can double check on this but i could be wrong here so correct me if i'm wrong in the comments thank you for that um right so how do we solve this issue well one of the things that we can do is we try different values writing is that we can start somewhere like here instead of starting at theta zero equals zero right or theta one equals zero right we can try random numbers to start with or we can change the step size right larger step size we could skip over the minimal right you know we can actually jump over it to avoid it and the thing is that the larger the step size the more inaccurate it is while the smaller the step size the more accurate it is but um it it takes longer time to optimize right um yeah so what are other things that we can use well we can also use a concept called stochastic gradient descent um which takes random points and then it basically picks a random well not here but like sorry about it takes a random data point right and then it calculates the derivative at that point and then it calculates the new values right and yeah so this avoids um well this causes randomness which makes it more likely that we're able to find the global minimum right and you can also use something called stock uh batch stock uh actually a mini batch stochastic gradient descent which takes a group of points instead of a single point here right um yeah it's super useful um so the special thing about um gradient descent is that we don't actually have to use uh sorry the special thing about linear regression is that we don't actually have to use gradient descent right we can also use something called the normal equation so what is the normal equation let me write it out normal equation so what does this do well um let me write out a variable x transpose x transpose um so what does this do well um what it does is that it it's a formula that we can plug in to find the best data value right um and so why don't we just use this instead of grading this set right well this actually works really well for smaller data sets you can use this for smaller data sets but as the data set gets a lot larger it will become um it will take a long time to calculate so that's why we use um gradient descent in practice but yeah um some interview questions might ask you on this but generally they'll ask you on gradient descent right and the normal equation only works for linear regression by the way you probably won't see it anywhere else for machine learning right and yeah x here is just independent variables and y is the dependent variable or the value we're trying to predict x could be you know house price uh house uh sorry house size number of swimming pools etc so now how do we um we i i only showed you how we can solve for the simple linear regression right simple linear regression we only have one feature right so what if we have like two features for example then we run into something called multiple layer regression and let me write that down so multiple linear regression instead we use the 3d point plane right now it could be number of rooms make sure you add arrows here says here you can see uh house size and then we're trying to predict the house price right so then um you know let me add some data points i'm going to make it larger to indicate that's closer and then smaller as it gets further right so how do i optimize for this well um instead of the line here we actually have like a plane oh god that's terrible i'm sorry yeah we have a plane here to best measure to best predictive values right and this is called multiple linear regression right all right so and that's with two or more values and if we only have uh one feature then it's just called simple linear regression um yeah and if we have more than two values we can't actually graphically um display this so that's why i'm going to be showing you with two but yeah it's the same thing we use gradient descent to optimize for all of them right you know it's just more features you know it could be theta three we just optimize for theta three using gradient descent and yeah i feel like that's really uh yeah all right cool i think that's it for this video if you enjoyed this um please subscribe i'm going to make more videos like this on whiteboard explaining things later um or maybe should i do animations what what do you guys think maybe whiteboard it's easier to explain things or maybe animations you guys decide in the comments below alright thank you for watching this video see you next time bye

